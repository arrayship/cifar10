{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","provenance":[],"collapsed_sections":["1Agb8w7qoAXI","nyGHTO07oFw2","80KsAgYYAEXa"],"mount_file_id":"1mG9Zej18lPY7sZiQu18VVxwsqamVjQZS","authorship_tag":"ABX9TyPoNM1+D+fbPWMr5oRmNsUF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"PqJseZbdUcSZ","colab_type":"text"},"source":["main\n","- checkpoint and early stopping (stop when train loss decrease & val loss increase)\n","- save trained model\n","- compare model test performance\n","- check logger system of ignite (+ plot)\n","\n","models\n","- efficientnet\n","\n","readme\n","- arrayship-style image classification neural network pytorch coding convention\n","    - 2 or more ways > block parallel\n","    - "]},{"cell_type":"markdown","metadata":{"id":"Qw7KzI-ywzDS","colab_type":"text"},"source":["## colab settings"]},{"cell_type":"code","metadata":{"id":"zT481u-xXJFB","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595172218938,"user_tz":-540,"elapsed":2154,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}}},"source":["import os\n","\n","prj_name = 'cifar-10'\n","prj_path = '/content/drive/My Drive/colab/study/image_classification/'\\\n","        + prj_name + '/'\n","os.chdir(prj_path + 'notebooks/')"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hq3OWTyviTQY","colab_type":"text"},"source":["## settings"]},{"cell_type":"code","metadata":{"id":"4lQ1Exrko3ht","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595172220850,"user_tz":-540,"elapsed":1675,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import sys\n","\n","sys.path.append('..')"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lpcd2kXHij01","colab_type":"text"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"WBWOqpQzPLXe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":181},"executionInfo":{"status":"ok","timestamp":1595172232375,"user_tz":-540,"elapsed":10935,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"ee32755d-f34a-4dc8-f18e-54e0b3f23fa8"},"source":["from importlib import import_module\n","\n","import torch\n","import torch.nn as nn\n","from torchvision import models\n","\n","!pip install pytorch-ignite\n","from ignite.metrics import Accuracy, Loss"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Collecting pytorch-ignite\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/86/69/811516d26518a204568bfbbd97838cff9d7ef4370684dcab49c6e5f60c5d/pytorch_ignite-0.4.0.post1-py2.py3-none-any.whl (164kB)\n","\r\u001b[K     |██                              | 10kB 22.3MB/s eta 0:00:01\r\u001b[K     |████                            | 20kB 5.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 30kB 6.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 40kB 7.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 51kB 6.6MB/s eta 0:00:01\r\u001b[K     |████████████                    | 61kB 7.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 71kB 7.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 81kB 8.3MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 92kB 8.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 174kB 8.4MB/s \n","\u001b[?25hRequirement already satisfied: torch<2,>=1.3 in /usr/local/lib/python3.6/dist-packages (from pytorch-ignite) (1.5.1+cu101)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch<2,>=1.3->pytorch-ignite) (1.18.5)\n","Installing collected packages: pytorch-ignite\n","Successfully installed pytorch-ignite-0.4.0.post1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"1Agb8w7qoAXI","colab_type":"text"},"source":["#### VGG"]},{"cell_type":"code","metadata":{"id":"8Di7AahiQ123","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.vgg'\n","model_name = 'VGG'\n","model_cfg = {'cfg': [[64], [128], [256, 256], [512, 512], [512, 512]],\n","        'batch_norm': True}\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyGHTO07oFw2","colab_type":"text"},"source":["#### InceptionV1"]},{"cell_type":"code","metadata":{"id":"RD_sPBtHoJi0","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.inception_v1'\n","model_name = 'InceptionV1'\n","model_cfg = [\n","        [{1: (0, 64), 3: (96, 128), 5: (16, 32), 'm': 32}, [False, 0, 0], None],\n","        [{1: (0, 128), 3: (128, 192), 5: (32, 96), 'm': 64}, [True, 3, 2], None],\n","        [{1: (0, 192), 3: (96, 208), 5: (16, 48), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 160), 3: (112, 224), 5: (24, 64), 'm': 64}, [False, 0, 0], 'aux'],\n","        [{1: (0, 128), 3: (128, 256), 5: (24, 64), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 112), 3: (144, 288), 5: (32, 64), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 256), 3: (160, 320), 5: (32, 128), 'm': 128}, [True, 2, 2], 'aux'],\n","        [{1: (0, 256), 3: (160, 320), 5: (32, 128), 'm': 128}, [False, 0, 0], None],\n","        [{1: (0, 384), 3: (192, 384), 5: (48, 128), 'm': 128}, [False, 0, 0], 'final']\n","        ]\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","class loss_cls(nn.Module):\n","    def __init__(self):\n","        super(loss_cls, self).__init__()\n","    def forward(self, inp, tar):\n","        loss = 0\n","        loss_fn = nn.CrossEntropyLoss()\n","        inps = inp.split(10, 1)\n","        for i, p in enumerate(inps):\n","            if i < len(inps) - 1:\n","                loss += 0.3 * loss_fn(p, tar)\n","            else:\n","                loss += 1.0 * loss_fn(p, tar)\n","        return loss\n","loss_fn = loss_cls()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80KsAgYYAEXa","colab_type":"text"},"source":["#### ResNet"]},{"cell_type":"code","metadata":{"id":"6D-Afz65AH4u","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.resnet'\n","model_name = 'ResNet'\n","model_cfg = [\n","        [(3, 64), (3, 64)],\n","        [(3, 64), (3, 64)],\n","        [(3, 128), (3, 128)],\n","        [(3, 128), (3, 128)],\n","        [(3, 256), (3, 256)],\n","        [(3, 256), (3, 256)],\n","        [(3, 512), (3, 512)],\n","        [(3, 512), (3, 512)]\n","        ]\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000\n","# startblock batchnorm eps check"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8jJVKBE_Xl3","colab_type":"text"},"source":["#### DenseNet"]},{"cell_type":"code","metadata":{"id":"s2Jt4rso_dzy","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595172624519,"user_tz":-540,"elapsed":1285,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}}},"source":["# model settings\n","model_mpath = 'src.models.densenet'\n","model_name = 'DenseNet'\n","gr = 4\n","cr = 0.5\n","model_cfg = {\n","        'start': 2 * gr,\n","        'dense': [\n","                [(4 * gr, gr)] * 4,\n","                [(4 * gr, gr)] * 8,\n","                [(4 * gr, gr)] * 12,\n","                #[(4 * gr, gr)] * 16\n","                ],\n","        'compress': cr\n","        }\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blvVm5EVLg-v","colab_type":"text"},"source":["## model construction"]},{"cell_type":"code","metadata":{"id":"-cWwjHvIGhV1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1595172627192,"user_tz":-540,"elapsed":1218,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"299e9a79-e5f6-4293-ec62-a9db30546a1c"},"source":["model_cls = getattr(\n","        import_module(model_mpath),\n","        model_name\n","        )\n","model = model_cls(model_cfg, init_weights=init_weights)\n","for b in model.named_children():\n","    print(b)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["('B_000', DNStartBlock(\n","  (B_000): Conv2d(3, 8, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (B_001): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_002): ReLU(inplace=True)\n","  (B_003): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","))\n","('B_001', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_002', DNTransitionBlock(\n","  (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_003', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_004): DNCompositeBlock(\n","    (B_000): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_005): DNCompositeBlock(\n","    (B_000): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_006): DNCompositeBlock(\n","    (B_000): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(36, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_007): DNCompositeBlock(\n","    (B_000): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_004', DNTransitionBlock(\n","  (B_000): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(44, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_005', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(22, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(26, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(30, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(34, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_004): DNCompositeBlock(\n","    (B_000): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(38, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_005): DNCompositeBlock(\n","    (B_000): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(42, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_006): DNCompositeBlock(\n","    (B_000): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(46, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_007): DNCompositeBlock(\n","    (B_000): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(50, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_008): DNCompositeBlock(\n","    (B_000): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(54, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_009): DNCompositeBlock(\n","    (B_000): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(58, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_010): DNCompositeBlock(\n","    (B_000): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(62, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_011): DNCompositeBlock(\n","    (B_000): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(66, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_006', DNTransitionBlock(\n","  (B_000): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(70, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_007', DNClfBlock(\n","  (B_000): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (B_001): Flatten()\n","  (B_002): Linear(in_features=35, out_features=10, bias=True)\n","))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iIbsjN3qFi50","colab_type":"text"},"source":["## load data"]},{"cell_type":"code","metadata":{"id":"vCMbJwLAL5UP","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595172276327,"user_tz":-540,"elapsed":13014,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}}},"source":["from src.data.make_dataset import Cifar10BatchDataset\n","\n","dpath = '../data/raw/'\n","\n","train_folds = []\n","for i in range(1, 6):\n","    train_folds.append(torch.load(dpath + 'data_batch_' + str(i) + '.pt'))\n","test_set = torch.load(dpath + 'test_batch.pt')"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbsLM8z4XB5X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":125},"executionInfo":{"status":"ok","timestamp":1595172276338,"user_tz":-540,"elapsed":12522,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"371ed8fe-3fde-423d-b8ec-749c81a9a04e"},"source":["from collections import Counter\n","\n","for train_fold in train_folds:\n","    print(sorted(Counter(s['label'] for s in train_fold).items()))\n","print(sorted(Counter(s['label'] for s in test_set).items()))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["[(0, 1005), (1, 974), (2, 1032), (3, 1016), (4, 999), (5, 937), (6, 1030), (7, 1001), (8, 1025), (9, 981)]\n","[(0, 984), (1, 1007), (2, 1010), (3, 995), (4, 1010), (5, 988), (6, 1008), (7, 1026), (8, 987), (9, 985)]\n","[(0, 994), (1, 1042), (2, 965), (3, 997), (4, 990), (5, 1029), (6, 978), (7, 1015), (8, 961), (9, 1029)]\n","[(0, 1003), (1, 963), (2, 1041), (3, 976), (4, 1004), (5, 1021), (6, 1004), (7, 981), (8, 1024), (9, 983)]\n","[(0, 1014), (1, 1014), (2, 952), (3, 1016), (4, 997), (5, 1025), (6, 980), (7, 977), (8, 1003), (9, 1022)]\n","[(0, 1000), (1, 1000), (2, 1000), (3, 1000), (4, 1000), (5, 1000), (6, 1000), (7, 1000), (8, 1000), (9, 1000)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8V8CmJvQWAV4","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1595172276342,"user_tz":-540,"elapsed":11901,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}}},"source":["from torch.utils.data import ConcatDataset, DataLoader\n","\n","train_set = ConcatDataset([f for j, f in enumerate(train_folds) if j != 4])\n","val_set = train_folds[4]\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","\n","del train_folds"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mw88rPC5Fizx","colab_type":"text"},"source":["## train"]},{"cell_type":"code","metadata":{"id":"__D0Iqs8WAg3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"3311e9ba-1732-4d33-ac49-98a49072e477"},"source":["from src.models.train_model import train_net\n","\n","opt = opt_(model.parameters(), lr)\n","\n","trainer = train_net(model, opt, loss_fn, val_metrics,\n","        train_loader, val_loader, device)\n","trainer.run(train_loader, max_epochs=max_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1\n","Train - acc: 0.11 loss: 2.33 \n","Val   - acc: 0.11 loss: 2.33 \n","Epoch 2\n","Train - acc: 0.12 loss: 2.32 \n","Val   - acc: 0.12 loss: 2.32 \n","Epoch 3\n","Train - acc: 0.13 loss: 2.31 \n","Val   - acc: 0.13 loss: 2.31 \n","Epoch 4\n","Train - acc: 0.14 loss: 2.30 \n","Val   - acc: 0.14 loss: 2.29 \n","Epoch 5\n","Train - acc: 0.14 loss: 2.28 \n","Val   - acc: 0.15 loss: 2.28 \n","Epoch 6\n","Train - acc: 0.15 loss: 2.27 \n","Val   - acc: 0.15 loss: 2.27 \n","Epoch 7\n","Train - acc: 0.15 loss: 2.26 \n","Val   - acc: 0.16 loss: 2.25 \n","Epoch 8\n","Train - acc: 0.16 loss: 2.24 \n","Val   - acc: 0.17 loss: 2.24 \n","Epoch 9\n","Train - acc: 0.17 loss: 2.23 \n","Val   - acc: 0.17 loss: 2.23 \n","Epoch 10\n","Train - acc: 0.17 loss: 2.22 \n","Val   - acc: 0.17 loss: 2.22 \n","Epoch 11\n","Train - acc: 0.18 loss: 2.21 \n","Val   - acc: 0.18 loss: 2.21 \n","Epoch 12\n","Train - acc: 0.18 loss: 2.20 \n","Val   - acc: 0.19 loss: 2.20 \n","Epoch 13\n","Train - acc: 0.19 loss: 2.18 \n","Val   - acc: 0.19 loss: 2.19 \n","Epoch 14\n","Train - acc: 0.19 loss: 2.17 \n","Val   - acc: 0.20 loss: 2.17 \n","Epoch 15\n","Train - acc: 0.20 loss: 2.16 \n","Val   - acc: 0.20 loss: 2.16 \n","Epoch 16\n","Train - acc: 0.20 loss: 2.15 \n","Val   - acc: 0.20 loss: 2.15 \n","Epoch 17\n","Train - acc: 0.21 loss: 2.14 \n","Val   - acc: 0.21 loss: 2.14 \n","Epoch 18\n","Train - acc: 0.21 loss: 2.13 \n","Val   - acc: 0.21 loss: 2.13 \n","Epoch 19\n","Train - acc: 0.22 loss: 2.12 \n","Val   - acc: 0.22 loss: 2.12 \n","Epoch 20\n","Train - acc: 0.22 loss: 2.11 \n","Val   - acc: 0.23 loss: 2.11 \n","Epoch 21\n","Train - acc: 0.23 loss: 2.10 \n","Val   - acc: 0.23 loss: 2.10 \n","Epoch 22\n","Train - acc: 0.23 loss: 2.09 \n","Val   - acc: 0.24 loss: 2.09 \n","Epoch 23\n","Train - acc: 0.24 loss: 2.08 \n","Val   - acc: 0.24 loss: 2.08 \n","Epoch 24\n","Train - acc: 0.24 loss: 2.07 \n","Val   - acc: 0.25 loss: 2.07 \n","Epoch 25\n","Train - acc: 0.25 loss: 2.06 \n","Val   - acc: 0.25 loss: 2.07 \n","Epoch 26\n","Train - acc: 0.25 loss: 2.05 \n","Val   - acc: 0.25 loss: 2.06 \n","Epoch 27\n","Train - acc: 0.26 loss: 2.04 \n","Val   - acc: 0.26 loss: 2.05 \n","Epoch 28\n","Train - acc: 0.26 loss: 2.03 \n","Val   - acc: 0.26 loss: 2.04 \n","Epoch 29\n","Train - acc: 0.26 loss: 2.02 \n","Val   - acc: 0.26 loss: 2.03 \n","Epoch 30\n","Train - acc: 0.27 loss: 2.01 \n","Val   - acc: 0.27 loss: 2.02 \n","Epoch 31\n","Train - acc: 0.27 loss: 2.00 \n","Val   - acc: 0.28 loss: 2.01 \n","Epoch 32\n","Train - acc: 0.28 loss: 1.99 \n","Val   - acc: 0.28 loss: 2.00 \n","Epoch 33\n","Train - acc: 0.28 loss: 1.98 \n","Val   - acc: 0.28 loss: 1.99 \n","Epoch 34\n","Train - acc: 0.28 loss: 1.97 \n","Val   - acc: 0.28 loss: 1.98 \n","Epoch 35\n","Train - acc: 0.29 loss: 1.97 \n","Val   - acc: 0.29 loss: 1.98 \n","Epoch 36\n","Train - acc: 0.29 loss: 1.96 \n","Val   - acc: 0.29 loss: 1.97 \n","Epoch 37\n","Train - acc: 0.30 loss: 1.95 \n","Val   - acc: 0.29 loss: 1.96 \n","Epoch 38\n","Train - acc: 0.30 loss: 1.94 \n","Val   - acc: 0.29 loss: 1.95 \n","Epoch 39\n","Train - acc: 0.30 loss: 1.93 \n","Val   - acc: 0.30 loss: 1.94 \n","Epoch 40\n","Train - acc: 0.30 loss: 1.92 \n","Val   - acc: 0.30 loss: 1.94 \n","Epoch 41\n","Train - acc: 0.31 loss: 1.92 \n","Val   - acc: 0.31 loss: 1.93 \n","Epoch 42\n","Train - acc: 0.31 loss: 1.91 \n","Val   - acc: 0.31 loss: 1.92 \n","Epoch 43\n","Train - acc: 0.31 loss: 1.90 \n","Val   - acc: 0.31 loss: 1.91 \n","Epoch 44\n","Train - acc: 0.32 loss: 1.89 \n","Val   - acc: 0.31 loss: 1.90 \n","Epoch 45\n","Train - acc: 0.32 loss: 1.88 \n","Val   - acc: 0.32 loss: 1.90 \n","Epoch 46\n","Train - acc: 0.32 loss: 1.88 \n","Val   - acc: 0.32 loss: 1.89 \n","Epoch 47\n","Train - acc: 0.32 loss: 1.87 \n","Val   - acc: 0.32 loss: 1.88 \n","Epoch 48\n","Train - acc: 0.33 loss: 1.86 \n","Val   - acc: 0.32 loss: 1.87 \n","Epoch 49\n","Train - acc: 0.33 loss: 1.85 \n","Val   - acc: 0.32 loss: 1.87 \n","Epoch 50\n","Train - acc: 0.33 loss: 1.84 \n","Val   - acc: 0.33 loss: 1.86 \n","Epoch 51\n","Train - acc: 0.33 loss: 1.84 \n","Val   - acc: 0.33 loss: 1.85 \n","Epoch 52\n","Train - acc: 0.34 loss: 1.83 \n","Val   - acc: 0.33 loss: 1.84 \n","Epoch 53\n","Train - acc: 0.34 loss: 1.82 \n","Val   - acc: 0.33 loss: 1.84 \n","Epoch 54\n","Train - acc: 0.34 loss: 1.82 \n","Val   - acc: 0.33 loss: 1.83 \n","Epoch 55\n","Train - acc: 0.34 loss: 1.81 \n","Val   - acc: 0.33 loss: 1.82 \n","Epoch 56\n","Train - acc: 0.34 loss: 1.80 \n","Val   - acc: 0.34 loss: 1.82 \n","Epoch 57\n","Train - acc: 0.35 loss: 1.80 \n","Val   - acc: 0.34 loss: 1.81 \n","Epoch 58\n","Train - acc: 0.35 loss: 1.79 \n","Val   - acc: 0.34 loss: 1.81 \n","Epoch 59\n","Train - acc: 0.35 loss: 1.79 \n","Val   - acc: 0.34 loss: 1.80 \n","Epoch 60\n","Train - acc: 0.35 loss: 1.78 \n","Val   - acc: 0.34 loss: 1.80 \n","Epoch 61\n","Train - acc: 0.35 loss: 1.78 \n","Val   - acc: 0.35 loss: 1.79 \n","Epoch 62\n","Train - acc: 0.35 loss: 1.77 \n","Val   - acc: 0.35 loss: 1.79 \n","Epoch 63\n","Train - acc: 0.36 loss: 1.76 \n","Val   - acc: 0.35 loss: 1.78 \n","Epoch 64\n","Train - acc: 0.36 loss: 1.76 \n","Val   - acc: 0.35 loss: 1.78 \n","Epoch 65\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.77 \n","Epoch 66\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.77 \n","Epoch 67\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 68\n","Train - acc: 0.36 loss: 1.74 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 69\n","Train - acc: 0.36 loss: 1.74 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 70\n","Train - acc: 0.37 loss: 1.73 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 71\n","Train - acc: 0.37 loss: 1.73 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 72\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 73\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.74 \n","Epoch 74\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.74 \n","Epoch 75\n","Train - acc: 0.37 loss: 1.71 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 76\n","Train - acc: 0.37 loss: 1.71 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 77\n","Train - acc: 0.37 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 78\n","Train - acc: 0.37 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 79\n","Train - acc: 0.38 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 80\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 81\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.36 loss: 1.71 \n","Epoch 82\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 83\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 84\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 85\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 86\n","Train - acc: 0.38 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 87\n","Train - acc: 0.39 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 88\n","Train - acc: 0.39 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 89\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 90\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 91\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 92\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 93\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 94\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 95\n","Train - acc: 0.39 loss: 1.64 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 96\n","Train - acc: 0.39 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 97\n","Train - acc: 0.40 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 98\n","Train - acc: 0.40 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 99\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 100\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 101\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 102\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 103\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.66 \n","Epoch 104\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 105\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 106\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 107\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 108\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 109\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 110\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 111\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 112\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 113\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 114\n","Train - acc: 0.41 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.64 \n","Epoch 115\n","Train - acc: 0.41 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 116\n","Train - acc: 0.42 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 117\n","Train - acc: 0.42 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 118\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 119\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 120\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 121\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 122\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 123\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 124\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 125\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 126\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 127\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 128\n","Train - acc: 0.42 loss: 1.56 \n","Val   - acc: 0.40 loss: 1.61 \n","Epoch 129\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 130\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 131\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 132\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 133\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 134\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 135\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 136\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 137\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 138\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 139\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 140\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 141\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 142\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 143\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 144\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 145\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 146\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 147\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 148\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 149\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 150\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 151\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 152\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 153\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 154\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 155\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 156\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 157\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 158\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 159\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 160\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 161\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 162\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 163\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 164\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 165\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 166\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 167\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 168\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 169\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 170\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 171\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 172\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 173\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"CJYQoYeFWApV","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"Er2I72bjiJXe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsuAz94ziJqZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NQ5P36ae5e7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPA6RFmLLugP","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m29760hLLuOi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Hz34O1UFinR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrtUBWcaFiih","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"woSchTbzFh0y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]}]}