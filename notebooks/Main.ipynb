{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Main.ipynb","provenance":[],"collapsed_sections":["1Agb8w7qoAXI","nyGHTO07oFw2","80KsAgYYAEXa"],"mount_file_id":"1mG9Zej18lPY7sZiQu18VVxwsqamVjQZS","authorship_tag":"ABX9TyNx9UAY4Kbi1rb4Glr+PUPj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"Qw7KzI-ywzDS","colab_type":"text"},"source":["## colab settings"]},{"cell_type":"code","metadata":{"id":"zT481u-xXJFB","colab_type":"code","colab":{}},"source":["import os\n","\n","prj_name = 'cifar-10'\n","prj_path = '/content/drive/My Drive/colab/study/image_classification/'\\\n","        + prj_name + '/'\n","os.chdir(prj_path + 'notebooks/')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hq3OWTyviTQY","colab_type":"text"},"source":["## settings"]},{"cell_type":"code","metadata":{"id":"4lQ1Exrko3ht","colab_type":"code","colab":{}},"source":["%load_ext autoreload\n","%autoreload 2\n","\n","import sys\n","\n","sys.path.append('..')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lpcd2kXHij01","colab_type":"text"},"source":["# main"]},{"cell_type":"code","metadata":{"id":"WBWOqpQzPLXe","colab_type":"code","colab":{}},"source":["from importlib import import_module\n","\n","import torch\n","import torch.nn as nn\n","\n","#!pip install pytorch-ignite\n","#from ignite.metrics import Accuracy, Loss"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1Agb8w7qoAXI","colab_type":"text"},"source":["#### VGG"]},{"cell_type":"code","metadata":{"id":"8Di7AahiQ123","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.vgg'\n","model_name = 'VGG'\n","model_cfg = {'cfg': [[64], [128], [256, 256], [512, 512], [512, 512]],\n","        'batch_norm': True}\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nyGHTO07oFw2","colab_type":"text"},"source":["#### InceptionV1"]},{"cell_type":"code","metadata":{"id":"RD_sPBtHoJi0","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.inception_v1'\n","model_name = 'InceptionV1'\n","model_cfg = [\n","        [{1: (0, 64), 3: (96, 128), 5: (16, 32), 'm': 32}, [False, 0, 0], None],\n","        [{1: (0, 128), 3: (128, 192), 5: (32, 96), 'm': 64}, [True, 3, 2], None],\n","        [{1: (0, 192), 3: (96, 208), 5: (16, 48), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 160), 3: (112, 224), 5: (24, 64), 'm': 64}, [False, 0, 0], 'aux'],\n","        [{1: (0, 128), 3: (128, 256), 5: (24, 64), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 112), 3: (144, 288), 5: (32, 64), 'm': 64}, [False, 0, 0], None],\n","        [{1: (0, 256), 3: (160, 320), 5: (32, 128), 'm': 128}, [True, 2, 2], 'aux'],\n","        [{1: (0, 256), 3: (160, 320), 5: (32, 128), 'm': 128}, [False, 0, 0], None],\n","        [{1: (0, 384), 3: (192, 384), 5: (48, 128), 'm': 128}, [False, 0, 0], 'final']\n","        ]\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","class loss_cls(nn.Module):\n","    def __init__(self):\n","        super(loss_cls, self).__init__()\n","    def forward(self, inp, tar):\n","        loss = 0\n","        loss_fn = nn.CrossEntropyLoss()\n","        inps = inp.split(10, 1)\n","        for i, p in enumerate(inps):\n","            if i < len(inps) - 1:\n","                loss += 0.3 * loss_fn(p, tar)\n","            else:\n","                loss += 1.0 * loss_fn(p, tar)\n","        return loss\n","loss_fn = loss_cls()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80KsAgYYAEXa","colab_type":"text"},"source":["#### ResNet"]},{"cell_type":"code","metadata":{"id":"6D-Afz65AH4u","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.resnet'\n","model_name = 'ResNet'\n","model_cfg = [\n","        [(3, 64), (3, 64)],\n","        [(3, 64), (3, 64)],\n","        [(3, 128), (3, 128)],\n","        [(3, 128), (3, 128)],\n","        [(3, 256), (3, 256)],\n","        [(3, 256), (3, 256)],\n","        [(3, 512), (3, 512)],\n","        [(3, 512), (3, 512)]\n","        ]\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000\n","# startblock batchnorm eps check"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r8jJVKBE_Xl3","colab_type":"text"},"source":["#### DenseNet"]},{"cell_type":"code","metadata":{"id":"s2Jt4rso_dzy","colab_type":"code","colab":{}},"source":["# model settings\n","model_mpath = 'src.models.densenet'\n","model_name = 'DenseNet'\n","gr = 4\n","cr = 0.5\n","model_cfg = {\n","        'start': 2 * gr,\n","        'dense': [\n","                [(4 * gr, gr)] * 4,\n","                [(4 * gr, gr)] * 8,\n","                [(4 * gr, gr)] * 12,\n","                #[(4 * gr, gr)] * 16\n","                ],\n","        'compress': cr\n","        }\n","init_weights = True\n","\n","# dataset, dataloader settings\n","batch_size = 2500\n","\n","# train settings\n","loss_fn = nn.CrossEntropyLoss()\n","opt_ = torch.optim.Adam\n","lr = 0.00003\n","#val_metrics = {'acc': Accuracy(), 'loss': Loss(loss_fn)}\n","device = 'cuda:0'\n","max_epochs = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"blvVm5EVLg-v","colab_type":"text"},"source":["## model construction"]},{"cell_type":"code","metadata":{"id":"-cWwjHvIGhV1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596463749219,"user_tz":-540,"elapsed":5669,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"efa7136f-5a92-4272-ac92-d8ad80bcc8fb"},"source":["model_cls = getattr(\n","        import_module(model_mpath),\n","        model_name\n","        )\n","model = model_cls(model_cfg, init_weights=init_weights)\n","for b in model.named_children():\n","    print(b)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["('B_000', DNStartBlock(\n","  (B_000): Conv2d(3, 8, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (B_001): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_002): ReLU(inplace=True)\n","  (B_003): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","))\n","('B_001', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_002', DNTransitionBlock(\n","  (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_003', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_004): DNCompositeBlock(\n","    (B_000): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_005): DNCompositeBlock(\n","    (B_000): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_006): DNCompositeBlock(\n","    (B_000): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(36, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_007): DNCompositeBlock(\n","    (B_000): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_004', DNTransitionBlock(\n","  (B_000): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(44, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_005', DNDenseBlock(\n","  (B_000): DNCompositeBlock(\n","    (B_000): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(22, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_001): DNCompositeBlock(\n","    (B_000): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(26, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_002): DNCompositeBlock(\n","    (B_000): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(30, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_003): DNCompositeBlock(\n","    (B_000): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(34, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_004): DNCompositeBlock(\n","    (B_000): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(38, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_005): DNCompositeBlock(\n","    (B_000): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(42, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_006): DNCompositeBlock(\n","    (B_000): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(46, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_007): DNCompositeBlock(\n","    (B_000): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(50, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_008): DNCompositeBlock(\n","    (B_000): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(54, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_009): DNCompositeBlock(\n","    (B_000): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(58, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_010): DNCompositeBlock(\n","    (B_000): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(62, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","  (B_011): DNCompositeBlock(\n","    (B_000): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_001): ReLU(inplace=True)\n","    (B_002): Conv2d(66, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","    (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    (B_004): ReLU(inplace=True)\n","    (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","  )\n","))\n","('B_006', DNTransitionBlock(\n","  (B_000): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (B_001): ReLU(inplace=True)\n","  (B_002): Conv2d(70, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","  (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","))\n","('B_007', DNClfBlock(\n","  (B_000): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (B_001): Flatten()\n","  (B_002): Linear(in_features=35, out_features=10, bias=True)\n","))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"iIbsjN3qFi50","colab_type":"text"},"source":["## load data"]},{"cell_type":"code","metadata":{"id":"vCMbJwLAL5UP","colab_type":"code","colab":{}},"source":["dpath = '../data/raw/'\n","\n","train_folds = []\n","for i in range(1, 6):\n","    train_folds.append(torch.load(dpath + 'data_batch_' + str(i) + '.pt'))\n","test_set = torch.load(dpath + 'test_batch.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UbsLM8z4XB5X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":145},"executionInfo":{"status":"ok","timestamp":1596463767017,"user_tz":-540,"elapsed":12400,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"33d0558e-d9ff-440a-d054-746aa8a30373"},"source":["from collections import Counter\n","\n","for train_fold in train_folds:\n","    print(sorted(Counter(s['label'] for s in train_fold).items()))\n","print(sorted(Counter(s['label'] for s in test_set).items()))"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[(0, 1005), (1, 974), (2, 1032), (3, 1016), (4, 999), (5, 937), (6, 1030), (7, 1001), (8, 1025), (9, 981)]\n","[(0, 984), (1, 1007), (2, 1010), (3, 995), (4, 1010), (5, 988), (6, 1008), (7, 1026), (8, 987), (9, 985)]\n","[(0, 994), (1, 1042), (2, 965), (3, 997), (4, 990), (5, 1029), (6, 978), (7, 1015), (8, 961), (9, 1029)]\n","[(0, 1003), (1, 963), (2, 1041), (3, 976), (4, 1004), (5, 1021), (6, 1004), (7, 981), (8, 1024), (9, 983)]\n","[(0, 1014), (1, 1014), (2, 952), (3, 1016), (4, 997), (5, 1025), (6, 980), (7, 977), (8, 1003), (9, 1022)]\n","[(0, 1000), (1, 1000), (2, 1000), (3, 1000), (4, 1000), (5, 1000), (6, 1000), (7, 1000), (8, 1000), (9, 1000)]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"8V8CmJvQWAV4","colab_type":"code","colab":{}},"source":["from torch.utils.data import ConcatDataset, DataLoader\n","\n","train_set = ConcatDataset([f for j, f in enumerate(train_folds) if j != 4])\n","val_set = train_folds[4]\n","\n","class SkorchDataset(torch.utils.data.Dataset):\n","    def __init__(self, ds):\n","        self.ds = ds\n","\n","    def __getitem__(self, idx):\n","        return tuple(self.ds[idx].values())\n","\n","    def __len__(self):\n","        return len(self.ds)\n","\n","train_set = SkorchDataset(train_set)\n","val_set = SkorchDataset(val_set)\n","\n","train_loader = DataLoader(train_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","val_loader = DataLoader(val_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","test_loader = DataLoader(test_set, batch_size=batch_size,\n","        shuffle=True, num_workers=2)\n","\n","del train_folds"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mw88rPC5Fizx","colab_type":"text"},"source":["## train"]},{"cell_type":"code","metadata":{"id":"75k-_D08rlfO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":431},"executionInfo":{"status":"ok","timestamp":1596464201096,"user_tz":-540,"elapsed":964,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"05662a33-08f6-4ba5-cc26-a2602878a07d"},"source":["train_set[0][0]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[0.2314, 0.1686, 0.1961,  ..., 0.6196, 0.5961, 0.5804],\n","         [0.0627, 0.0000, 0.0706,  ..., 0.4824, 0.4667, 0.4784],\n","         [0.0980, 0.0627, 0.1922,  ..., 0.4627, 0.4706, 0.4275],\n","         ...,\n","         [0.8157, 0.7882, 0.7765,  ..., 0.6275, 0.2196, 0.2078],\n","         [0.7059, 0.6784, 0.7294,  ..., 0.7216, 0.3804, 0.3255],\n","         [0.6941, 0.6588, 0.7020,  ..., 0.8471, 0.5922, 0.4824]],\n","\n","        [[0.2431, 0.1804, 0.1882,  ..., 0.5176, 0.4902, 0.4863],\n","         [0.0784, 0.0000, 0.0314,  ..., 0.3451, 0.3255, 0.3412],\n","         [0.0941, 0.0275, 0.1059,  ..., 0.3294, 0.3294, 0.2863],\n","         ...,\n","         [0.6667, 0.6000, 0.6314,  ..., 0.5216, 0.1216, 0.1333],\n","         [0.5451, 0.4824, 0.5647,  ..., 0.5804, 0.2431, 0.2078],\n","         [0.5647, 0.5059, 0.5569,  ..., 0.7216, 0.4627, 0.3608]],\n","\n","        [[0.2471, 0.1765, 0.1686,  ..., 0.4235, 0.4000, 0.4039],\n","         [0.0784, 0.0000, 0.0000,  ..., 0.2157, 0.1961, 0.2235],\n","         [0.0824, 0.0000, 0.0314,  ..., 0.1961, 0.1961, 0.1647],\n","         ...,\n","         [0.3765, 0.1333, 0.1020,  ..., 0.2745, 0.0275, 0.0784],\n","         [0.3765, 0.1647, 0.1176,  ..., 0.3686, 0.1333, 0.1333],\n","         [0.4549, 0.3686, 0.3412,  ..., 0.5490, 0.3294, 0.2824]]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"code","metadata":{"id":"RXslFNNO7CPg","colab_type":"code","colab":{}},"source":["%%capture\n","!pip install skorch\n","\n","from skorch.net import NeuralNet\n","from skorch.helper import predefined_split\n","from skorch.callbacks import EpochScoring\n","\n","from sklearn.utils.multiclass import type_of_target\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","def argmax_acc(skorch_model, ds, y=None):\n","    y_true = [y for _, y in ds]\n","    y_pred = skorch_model.predict(ds).argmax(1)\n","    return accuracy_score(y_true, y_pred)\n","train_acc_cb = EpochScoring(argmax_acc, lower_is_better=False,\n","        on_train=True, name='train_acc') # history에서 불러오는 뭐시기가 있대, 10epoch마다 보이기?\n","val_acc_cb = EpochScoring(argmax_acc, lower_is_better=False,\n","        name='valid_acc')\n","\n","skorch_model = NeuralNet(\n","        model,\n","        nn.CrossEntropyLoss,\n","        opt_,\n","        lr,\n","        max_epochs,\n","        batch_size,\n","        train_split=predefined_split(val_set),\n","        callbacks=[train_acc_cb, val_acc_cb],\n","        device=device\n","        )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"egjjd1hX7uQS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"ok","timestamp":1596466774138,"user_tz":-540,"elapsed":652977,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"3b3f5484-f49a-447d-a0b0-7c8f932e77ac"},"source":["skorch_model.fit(train_set)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["  epoch    train_acc    train_loss    val_acc    valid_loss     dur\n","-------  -----------  ------------  ---------  ------------  ------\n","      1       \u001b[36m0.2425\u001b[0m        \u001b[32m2.0852\u001b[0m     \u001b[35m0.2340\u001b[0m        \u001b[31m2.0932\u001b[0m  3.0215\n","      2       \u001b[36m0.2453\u001b[0m        \u001b[32m2.0763\u001b[0m     \u001b[35m0.2373\u001b[0m        \u001b[31m2.0851\u001b[0m  2.7635\n","      3       \u001b[36m0.2479\u001b[0m        \u001b[32m2.0677\u001b[0m     \u001b[35m0.2413\u001b[0m        \u001b[31m2.0770\u001b[0m  2.7546\n","      4       \u001b[36m0.2511\u001b[0m        \u001b[32m2.0591\u001b[0m     \u001b[35m0.2453\u001b[0m        \u001b[31m2.0689\u001b[0m  2.8692\n","      5       \u001b[36m0.2548\u001b[0m        \u001b[32m2.0507\u001b[0m     \u001b[35m0.2488\u001b[0m        \u001b[31m2.0613\u001b[0m  2.7533\n","      6       \u001b[36m0.2576\u001b[0m        \u001b[32m2.0425\u001b[0m     \u001b[35m0.2510\u001b[0m        \u001b[31m2.0533\u001b[0m  2.7470\n","      7       \u001b[36m0.2611\u001b[0m        \u001b[32m2.0344\u001b[0m     \u001b[35m0.2525\u001b[0m        \u001b[31m2.0458\u001b[0m  2.7601\n","      8       \u001b[36m0.2635\u001b[0m        \u001b[32m2.0265\u001b[0m     \u001b[35m0.2544\u001b[0m        \u001b[31m2.0385\u001b[0m  2.7429\n","      9       \u001b[36m0.2664\u001b[0m        \u001b[32m2.0187\u001b[0m     \u001b[35m0.2581\u001b[0m        \u001b[31m2.0308\u001b[0m  2.8740\n","     10       \u001b[36m0.2692\u001b[0m        \u001b[32m2.0111\u001b[0m     \u001b[35m0.2615\u001b[0m        \u001b[31m2.0235\u001b[0m  2.7271\n","     11       \u001b[36m0.2722\u001b[0m        \u001b[32m2.0036\u001b[0m     \u001b[35m0.2646\u001b[0m        \u001b[31m2.0162\u001b[0m  2.7587\n","     12       \u001b[36m0.2759\u001b[0m        \u001b[32m1.9961\u001b[0m     \u001b[35m0.2694\u001b[0m        \u001b[31m2.0086\u001b[0m  2.7312\n","     13       \u001b[36m0.2789\u001b[0m        \u001b[32m1.9889\u001b[0m     \u001b[35m0.2713\u001b[0m        \u001b[31m2.0015\u001b[0m  2.8402\n","     14       \u001b[36m0.2821\u001b[0m        \u001b[32m1.9817\u001b[0m     \u001b[35m0.2742\u001b[0m        \u001b[31m1.9949\u001b[0m  2.7269\n","     15       \u001b[36m0.2853\u001b[0m        \u001b[32m1.9746\u001b[0m     0.2739        \u001b[31m1.9883\u001b[0m  2.7322\n","     16       \u001b[36m0.2879\u001b[0m        \u001b[32m1.9676\u001b[0m     \u001b[35m0.2748\u001b[0m        \u001b[31m1.9814\u001b[0m  2.7398\n","     17       \u001b[36m0.2900\u001b[0m        \u001b[32m1.9607\u001b[0m     \u001b[35m0.2797\u001b[0m        \u001b[31m1.9746\u001b[0m  2.8585\n","     18       \u001b[36m0.2931\u001b[0m        \u001b[32m1.9539\u001b[0m     \u001b[35m0.2828\u001b[0m        \u001b[31m1.9678\u001b[0m  2.7373\n","     19       \u001b[36m0.2953\u001b[0m        \u001b[32m1.9471\u001b[0m     \u001b[35m0.2845\u001b[0m        \u001b[31m1.9612\u001b[0m  2.7473\n","     20       \u001b[36m0.2981\u001b[0m        \u001b[32m1.9404\u001b[0m     \u001b[35m0.2880\u001b[0m        \u001b[31m1.9541\u001b[0m  2.8562\n","     21       \u001b[36m0.3005\u001b[0m        \u001b[32m1.9338\u001b[0m     \u001b[35m0.2926\u001b[0m        \u001b[31m1.9473\u001b[0m  2.7347\n","     22       \u001b[36m0.3023\u001b[0m        \u001b[32m1.9273\u001b[0m     \u001b[35m0.2966\u001b[0m        \u001b[31m1.9410\u001b[0m  2.7294\n","     23       \u001b[36m0.3048\u001b[0m        \u001b[32m1.9210\u001b[0m     \u001b[35m0.2991\u001b[0m        \u001b[31m1.9342\u001b[0m  2.7351\n","     24       \u001b[36m0.3071\u001b[0m        \u001b[32m1.9147\u001b[0m     \u001b[35m0.3023\u001b[0m        \u001b[31m1.9282\u001b[0m  2.7473\n","     25       \u001b[36m0.3092\u001b[0m        \u001b[32m1.9085\u001b[0m     \u001b[35m0.3050\u001b[0m        \u001b[31m1.9218\u001b[0m  2.8535\n","     26       \u001b[36m0.3114\u001b[0m        \u001b[32m1.9023\u001b[0m     \u001b[35m0.3074\u001b[0m        \u001b[31m1.9153\u001b[0m  2.7612\n","     27       \u001b[36m0.3136\u001b[0m        \u001b[32m1.8963\u001b[0m     \u001b[35m0.3088\u001b[0m        \u001b[31m1.9095\u001b[0m  2.7485\n","     28       \u001b[36m0.3167\u001b[0m        \u001b[32m1.8902\u001b[0m     \u001b[35m0.3104\u001b[0m        \u001b[31m1.9032\u001b[0m  2.7476\n","     29       \u001b[36m0.3183\u001b[0m        \u001b[32m1.8843\u001b[0m     \u001b[35m0.3112\u001b[0m        \u001b[31m1.8971\u001b[0m  2.8558\n","     30       \u001b[36m0.3207\u001b[0m        \u001b[32m1.8783\u001b[0m     \u001b[35m0.3131\u001b[0m        \u001b[31m1.8915\u001b[0m  2.7526\n","     31       \u001b[36m0.3228\u001b[0m        \u001b[32m1.8725\u001b[0m     \u001b[35m0.3137\u001b[0m        \u001b[31m1.8862\u001b[0m  2.7517\n","     32       \u001b[36m0.3245\u001b[0m        \u001b[32m1.8667\u001b[0m     \u001b[35m0.3151\u001b[0m        \u001b[31m1.8811\u001b[0m  2.8600\n","     33       \u001b[36m0.3269\u001b[0m        \u001b[32m1.8610\u001b[0m     \u001b[35m0.3177\u001b[0m        \u001b[31m1.8755\u001b[0m  2.7431\n","     34       \u001b[36m0.3289\u001b[0m        \u001b[32m1.8554\u001b[0m     \u001b[35m0.3204\u001b[0m        \u001b[31m1.8698\u001b[0m  2.7588\n","     35       \u001b[36m0.3302\u001b[0m        \u001b[32m1.8498\u001b[0m     \u001b[35m0.3233\u001b[0m        \u001b[31m1.8646\u001b[0m  2.7412\n","     36       \u001b[36m0.3317\u001b[0m        \u001b[32m1.8443\u001b[0m     \u001b[35m0.3236\u001b[0m        \u001b[31m1.8592\u001b[0m  2.8513\n","     37       \u001b[36m0.3334\u001b[0m        \u001b[32m1.8390\u001b[0m     \u001b[35m0.3250\u001b[0m        \u001b[31m1.8536\u001b[0m  2.7367\n","     38       \u001b[36m0.3350\u001b[0m        \u001b[32m1.8337\u001b[0m     \u001b[35m0.3253\u001b[0m        \u001b[31m1.8487\u001b[0m  2.7573\n","     39       \u001b[36m0.3372\u001b[0m        \u001b[32m1.8284\u001b[0m     \u001b[35m0.3261\u001b[0m        \u001b[31m1.8439\u001b[0m  2.7354\n","     40       \u001b[36m0.3382\u001b[0m        \u001b[32m1.8232\u001b[0m     \u001b[35m0.3279\u001b[0m        \u001b[31m1.8389\u001b[0m  2.7543\n","     41       \u001b[36m0.3396\u001b[0m        \u001b[32m1.8181\u001b[0m     \u001b[35m0.3287\u001b[0m        \u001b[31m1.8340\u001b[0m  2.8479\n","     42       \u001b[36m0.3410\u001b[0m        \u001b[32m1.8131\u001b[0m     \u001b[35m0.3303\u001b[0m        \u001b[31m1.8292\u001b[0m  2.7336\n","     43       \u001b[36m0.3424\u001b[0m        \u001b[32m1.8083\u001b[0m     \u001b[35m0.3334\u001b[0m        \u001b[31m1.8244\u001b[0m  2.7472\n","     44       \u001b[36m0.3437\u001b[0m        \u001b[32m1.8035\u001b[0m     \u001b[35m0.3352\u001b[0m        \u001b[31m1.8198\u001b[0m  2.8641\n","     45       \u001b[36m0.3452\u001b[0m        \u001b[32m1.7988\u001b[0m     \u001b[35m0.3358\u001b[0m        \u001b[31m1.8156\u001b[0m  2.7485\n","     46       \u001b[36m0.3462\u001b[0m        \u001b[32m1.7941\u001b[0m     \u001b[35m0.3372\u001b[0m        \u001b[31m1.8112\u001b[0m  2.7562\n","     47       \u001b[36m0.3483\u001b[0m        \u001b[32m1.7895\u001b[0m     \u001b[35m0.3387\u001b[0m        \u001b[31m1.8070\u001b[0m  2.7450\n","     48       \u001b[36m0.3496\u001b[0m        \u001b[32m1.7850\u001b[0m     \u001b[35m0.3389\u001b[0m        \u001b[31m1.8028\u001b[0m  2.8507\n","     49       \u001b[36m0.3504\u001b[0m        \u001b[32m1.7806\u001b[0m     \u001b[35m0.3403\u001b[0m        \u001b[31m1.7987\u001b[0m  2.7418\n","     50       \u001b[36m0.3514\u001b[0m        \u001b[32m1.7763\u001b[0m     \u001b[35m0.3419\u001b[0m        \u001b[31m1.7948\u001b[0m  2.7348\n","     51       \u001b[36m0.3529\u001b[0m        \u001b[32m1.7720\u001b[0m     \u001b[35m0.3425\u001b[0m        \u001b[31m1.7911\u001b[0m  2.7310\n","     52       \u001b[36m0.3545\u001b[0m        \u001b[32m1.7678\u001b[0m     \u001b[35m0.3448\u001b[0m        \u001b[31m1.7873\u001b[0m  2.8447\n","     53       \u001b[36m0.3559\u001b[0m        \u001b[32m1.7637\u001b[0m     \u001b[35m0.3466\u001b[0m        \u001b[31m1.7835\u001b[0m  2.7409\n","     54       \u001b[36m0.3568\u001b[0m        \u001b[32m1.7597\u001b[0m     \u001b[35m0.3485\u001b[0m        \u001b[31m1.7798\u001b[0m  2.7438\n","     55       \u001b[36m0.3577\u001b[0m        \u001b[32m1.7557\u001b[0m     \u001b[35m0.3486\u001b[0m        \u001b[31m1.7762\u001b[0m  2.7413\n","     56       \u001b[36m0.3584\u001b[0m        \u001b[32m1.7518\u001b[0m     \u001b[35m0.3506\u001b[0m        \u001b[31m1.7727\u001b[0m  2.8494\n","     57       \u001b[36m0.3593\u001b[0m        \u001b[32m1.7479\u001b[0m     \u001b[35m0.3522\u001b[0m        \u001b[31m1.7694\u001b[0m  2.7383\n","     58       \u001b[36m0.3604\u001b[0m        \u001b[32m1.7442\u001b[0m     \u001b[35m0.3532\u001b[0m        \u001b[31m1.7662\u001b[0m  2.7325\n","     59       \u001b[36m0.3613\u001b[0m        \u001b[32m1.7405\u001b[0m     \u001b[35m0.3546\u001b[0m        \u001b[31m1.7630\u001b[0m  2.7491\n","     60       \u001b[36m0.3622\u001b[0m        \u001b[32m1.7370\u001b[0m     \u001b[35m0.3558\u001b[0m        \u001b[31m1.7598\u001b[0m  2.7442\n","     61       \u001b[36m0.3630\u001b[0m        \u001b[32m1.7334\u001b[0m     \u001b[35m0.3566\u001b[0m        \u001b[31m1.7566\u001b[0m  2.7460\n","     62       \u001b[36m0.3639\u001b[0m        \u001b[32m1.7300\u001b[0m     \u001b[35m0.3580\u001b[0m        \u001b[31m1.7534\u001b[0m  2.7439\n","     63       \u001b[36m0.3649\u001b[0m        \u001b[32m1.7265\u001b[0m     \u001b[35m0.3582\u001b[0m        \u001b[31m1.7506\u001b[0m  2.8416\n","     64       \u001b[36m0.3662\u001b[0m        \u001b[32m1.7231\u001b[0m     \u001b[35m0.3584\u001b[0m        \u001b[31m1.7477\u001b[0m  2.7429\n","     65       \u001b[36m0.3676\u001b[0m        \u001b[32m1.7197\u001b[0m     \u001b[35m0.3607\u001b[0m        \u001b[31m1.7446\u001b[0m  2.7371\n","     66       \u001b[36m0.3685\u001b[0m        \u001b[32m1.7164\u001b[0m     \u001b[35m0.3611\u001b[0m        \u001b[31m1.7420\u001b[0m  2.7330\n","     67       \u001b[36m0.3691\u001b[0m        \u001b[32m1.7132\u001b[0m     \u001b[35m0.3620\u001b[0m        \u001b[31m1.7393\u001b[0m  2.8407\n","     68       \u001b[36m0.3699\u001b[0m        \u001b[32m1.7100\u001b[0m     \u001b[35m0.3630\u001b[0m        \u001b[31m1.7366\u001b[0m  2.7319\n","     69       \u001b[36m0.3714\u001b[0m        \u001b[32m1.7068\u001b[0m     \u001b[35m0.3635\u001b[0m        \u001b[31m1.7339\u001b[0m  2.7500\n","     70       \u001b[36m0.3728\u001b[0m        \u001b[32m1.7037\u001b[0m     \u001b[35m0.3641\u001b[0m        \u001b[31m1.7313\u001b[0m  2.7385\n","     71       \u001b[36m0.3734\u001b[0m        \u001b[32m1.7007\u001b[0m     \u001b[35m0.3645\u001b[0m        \u001b[31m1.7287\u001b[0m  2.8324\n","     72       \u001b[36m0.3740\u001b[0m        \u001b[32m1.6977\u001b[0m     0.3645        \u001b[31m1.7264\u001b[0m  2.7282\n","     73       \u001b[36m0.3750\u001b[0m        \u001b[32m1.6947\u001b[0m     \u001b[35m0.3655\u001b[0m        \u001b[31m1.7241\u001b[0m  2.7435\n","     74       \u001b[36m0.3757\u001b[0m        \u001b[32m1.6918\u001b[0m     \u001b[35m0.3657\u001b[0m        \u001b[31m1.7217\u001b[0m  2.8665\n","     75       \u001b[36m0.3770\u001b[0m        \u001b[32m1.6889\u001b[0m     \u001b[35m0.3670\u001b[0m        \u001b[31m1.7195\u001b[0m  2.7427\n","     76       \u001b[36m0.3776\u001b[0m        \u001b[32m1.6861\u001b[0m     0.3664        \u001b[31m1.7172\u001b[0m  2.7294\n","     77       \u001b[36m0.3784\u001b[0m        \u001b[32m1.6832\u001b[0m     0.3668        \u001b[31m1.7150\u001b[0m  2.7489\n","     78       \u001b[36m0.3797\u001b[0m        \u001b[32m1.6804\u001b[0m     \u001b[35m0.3672\u001b[0m        \u001b[31m1.7129\u001b[0m  2.8414\n","     79       \u001b[36m0.3805\u001b[0m        \u001b[32m1.6777\u001b[0m     \u001b[35m0.3677\u001b[0m        \u001b[31m1.7107\u001b[0m  2.7339\n","     80       \u001b[36m0.3812\u001b[0m        \u001b[32m1.6749\u001b[0m     0.3674        \u001b[31m1.7085\u001b[0m  2.7481\n","     81       \u001b[36m0.3817\u001b[0m        \u001b[32m1.6722\u001b[0m     \u001b[35m0.3684\u001b[0m        \u001b[31m1.7064\u001b[0m  2.8453\n","     82       \u001b[36m0.3827\u001b[0m        \u001b[32m1.6696\u001b[0m     \u001b[35m0.3689\u001b[0m        \u001b[31m1.7046\u001b[0m  2.9379\n","     83       \u001b[36m0.3839\u001b[0m        \u001b[32m1.6670\u001b[0m     \u001b[35m0.3694\u001b[0m        \u001b[31m1.7025\u001b[0m  2.7352\n","     84       \u001b[36m0.3848\u001b[0m        \u001b[32m1.6644\u001b[0m     \u001b[35m0.3706\u001b[0m        \u001b[31m1.7007\u001b[0m  2.7204\n","     85       \u001b[36m0.3862\u001b[0m        \u001b[32m1.6618\u001b[0m     \u001b[35m0.3719\u001b[0m        \u001b[31m1.6986\u001b[0m  2.7354\n","     86       \u001b[36m0.3875\u001b[0m        \u001b[32m1.6593\u001b[0m     0.3714        \u001b[31m1.6967\u001b[0m  2.7394\n","     87       \u001b[36m0.3891\u001b[0m        \u001b[32m1.6567\u001b[0m     \u001b[35m0.3721\u001b[0m        \u001b[31m1.6948\u001b[0m  2.8523\n","     88       \u001b[36m0.3893\u001b[0m        \u001b[32m1.6543\u001b[0m     \u001b[35m0.3726\u001b[0m        \u001b[31m1.6928\u001b[0m  2.7415\n","     89       \u001b[36m0.3899\u001b[0m        \u001b[32m1.6518\u001b[0m     \u001b[35m0.3731\u001b[0m        \u001b[31m1.6910\u001b[0m  2.7432\n","     90       \u001b[36m0.3907\u001b[0m        \u001b[32m1.6494\u001b[0m     \u001b[35m0.3736\u001b[0m        \u001b[31m1.6891\u001b[0m  2.8621\n","     91       \u001b[36m0.3916\u001b[0m        \u001b[32m1.6469\u001b[0m     \u001b[35m0.3750\u001b[0m        \u001b[31m1.6875\u001b[0m  2.7316\n","     92       \u001b[36m0.3928\u001b[0m        \u001b[32m1.6446\u001b[0m     \u001b[35m0.3764\u001b[0m        \u001b[31m1.6855\u001b[0m  2.7471\n","     93       \u001b[36m0.3936\u001b[0m        \u001b[32m1.6422\u001b[0m     \u001b[35m0.3766\u001b[0m        \u001b[31m1.6838\u001b[0m  2.7397\n","     94       \u001b[36m0.3945\u001b[0m        \u001b[32m1.6399\u001b[0m     \u001b[35m0.3785\u001b[0m        \u001b[31m1.6820\u001b[0m  2.8495\n","     95       \u001b[36m0.3949\u001b[0m        \u001b[32m1.6376\u001b[0m     \u001b[35m0.3790\u001b[0m        \u001b[31m1.6803\u001b[0m  2.7494\n","     96       \u001b[36m0.3951\u001b[0m        \u001b[32m1.6353\u001b[0m     0.3790        \u001b[31m1.6787\u001b[0m  2.7773\n","     97       \u001b[36m0.3957\u001b[0m        \u001b[32m1.6330\u001b[0m     \u001b[35m0.3795\u001b[0m        \u001b[31m1.6769\u001b[0m  2.7326\n","     98       \u001b[36m0.3963\u001b[0m        \u001b[32m1.6308\u001b[0m     \u001b[35m0.3806\u001b[0m        \u001b[31m1.6754\u001b[0m  2.8533\n","     99       \u001b[36m0.3972\u001b[0m        \u001b[32m1.6286\u001b[0m     \u001b[35m0.3814\u001b[0m        \u001b[31m1.6737\u001b[0m  2.7396\n","    100       \u001b[36m0.3985\u001b[0m        \u001b[32m1.6264\u001b[0m     \u001b[35m0.3823\u001b[0m        \u001b[31m1.6722\u001b[0m  2.7461\n","    101       \u001b[36m0.3995\u001b[0m        \u001b[32m1.6242\u001b[0m     \u001b[35m0.3836\u001b[0m        \u001b[31m1.6706\u001b[0m  2.7445\n","    102       \u001b[36m0.4004\u001b[0m        \u001b[32m1.6220\u001b[0m     \u001b[35m0.3845\u001b[0m        \u001b[31m1.6691\u001b[0m  2.8632\n","    103       \u001b[36m0.4016\u001b[0m        \u001b[32m1.6199\u001b[0m     \u001b[35m0.3848\u001b[0m        \u001b[31m1.6675\u001b[0m  2.7538\n","    104       \u001b[36m0.4024\u001b[0m        \u001b[32m1.6177\u001b[0m     \u001b[35m0.3859\u001b[0m        \u001b[31m1.6661\u001b[0m  2.7598\n","    105       \u001b[36m0.4028\u001b[0m        \u001b[32m1.6156\u001b[0m     \u001b[35m0.3864\u001b[0m        \u001b[31m1.6646\u001b[0m  2.8503\n","    106       \u001b[36m0.4042\u001b[0m        \u001b[32m1.6135\u001b[0m     \u001b[35m0.3869\u001b[0m        \u001b[31m1.6631\u001b[0m  2.7298\n","    107       \u001b[36m0.4044\u001b[0m        \u001b[32m1.6114\u001b[0m     \u001b[35m0.3881\u001b[0m        \u001b[31m1.6618\u001b[0m  2.7467\n","    108       \u001b[36m0.4055\u001b[0m        \u001b[32m1.6093\u001b[0m     \u001b[35m0.3888\u001b[0m        \u001b[31m1.6605\u001b[0m  2.7280\n","    109       \u001b[36m0.4065\u001b[0m        \u001b[32m1.6073\u001b[0m     0.3888        \u001b[31m1.6590\u001b[0m  2.8422\n","    110       \u001b[36m0.4071\u001b[0m        \u001b[32m1.6052\u001b[0m     \u001b[35m0.3894\u001b[0m        \u001b[31m1.6576\u001b[0m  2.7452\n","    111       \u001b[36m0.4082\u001b[0m        \u001b[32m1.6032\u001b[0m     \u001b[35m0.3904\u001b[0m        \u001b[31m1.6563\u001b[0m  2.7330\n","    112       \u001b[36m0.4088\u001b[0m        \u001b[32m1.6013\u001b[0m     \u001b[35m0.3908\u001b[0m        \u001b[31m1.6549\u001b[0m  2.7463\n","    113       \u001b[36m0.4095\u001b[0m        \u001b[32m1.5993\u001b[0m     0.3904        \u001b[31m1.6536\u001b[0m  2.8614\n","    114       \u001b[36m0.4101\u001b[0m        \u001b[32m1.5974\u001b[0m     0.3907        \u001b[31m1.6523\u001b[0m  2.7522\n","    115       \u001b[36m0.4106\u001b[0m        \u001b[32m1.5955\u001b[0m     \u001b[35m0.3909\u001b[0m        \u001b[31m1.6509\u001b[0m  2.7265\n","    116       \u001b[36m0.4111\u001b[0m        \u001b[32m1.5936\u001b[0m     \u001b[35m0.3920\u001b[0m        \u001b[31m1.6496\u001b[0m  2.7475\n","    117       \u001b[36m0.4119\u001b[0m        \u001b[32m1.5918\u001b[0m     \u001b[35m0.3924\u001b[0m        \u001b[31m1.6483\u001b[0m  2.8586\n","    118       \u001b[36m0.4129\u001b[0m        \u001b[32m1.5899\u001b[0m     0.3924        \u001b[31m1.6471\u001b[0m  2.7406\n","    119       \u001b[36m0.4135\u001b[0m        \u001b[32m1.5881\u001b[0m     0.3924        \u001b[31m1.6459\u001b[0m  2.7384\n","    120       \u001b[36m0.4142\u001b[0m        \u001b[32m1.5863\u001b[0m     \u001b[35m0.3932\u001b[0m        \u001b[31m1.6447\u001b[0m  2.8530\n","    121       \u001b[36m0.4148\u001b[0m        \u001b[32m1.5846\u001b[0m     \u001b[35m0.3933\u001b[0m        \u001b[31m1.6435\u001b[0m  2.7360\n","    122       \u001b[36m0.4157\u001b[0m        \u001b[32m1.5828\u001b[0m     \u001b[35m0.3934\u001b[0m        \u001b[31m1.6423\u001b[0m  2.7347\n","    123       \u001b[36m0.4163\u001b[0m        \u001b[32m1.5811\u001b[0m     \u001b[35m0.3941\u001b[0m        \u001b[31m1.6410\u001b[0m  2.7401\n","    124       \u001b[36m0.4169\u001b[0m        \u001b[32m1.5794\u001b[0m     \u001b[35m0.3943\u001b[0m        \u001b[31m1.6398\u001b[0m  2.8614\n","    125       \u001b[36m0.4178\u001b[0m        \u001b[32m1.5776\u001b[0m     \u001b[35m0.3953\u001b[0m        \u001b[31m1.6385\u001b[0m  2.7321\n","    126       \u001b[36m0.4184\u001b[0m        \u001b[32m1.5759\u001b[0m     \u001b[35m0.3962\u001b[0m        \u001b[31m1.6374\u001b[0m  2.7398\n","    127       \u001b[36m0.4190\u001b[0m        \u001b[32m1.5742\u001b[0m     \u001b[35m0.3966\u001b[0m        \u001b[31m1.6363\u001b[0m  2.7330\n","    128       \u001b[36m0.4198\u001b[0m        \u001b[32m1.5725\u001b[0m     \u001b[35m0.3967\u001b[0m        \u001b[31m1.6352\u001b[0m  2.8438\n","    129       \u001b[36m0.4204\u001b[0m        \u001b[32m1.5709\u001b[0m     0.3967        \u001b[31m1.6340\u001b[0m  2.7499\n","    130       \u001b[36m0.4208\u001b[0m        \u001b[32m1.5692\u001b[0m     \u001b[35m0.3975\u001b[0m        \u001b[31m1.6329\u001b[0m  2.7340\n","    131       \u001b[36m0.4215\u001b[0m        \u001b[32m1.5676\u001b[0m     0.3975        \u001b[31m1.6319\u001b[0m  2.8531\n","    132       \u001b[36m0.4217\u001b[0m        \u001b[32m1.5660\u001b[0m     \u001b[35m0.3980\u001b[0m        \u001b[31m1.6307\u001b[0m  2.7337\n","    133       \u001b[36m0.4225\u001b[0m        \u001b[32m1.5644\u001b[0m     \u001b[35m0.3985\u001b[0m        \u001b[31m1.6297\u001b[0m  2.7367\n","    134       \u001b[36m0.4227\u001b[0m        \u001b[32m1.5628\u001b[0m     \u001b[35m0.3996\u001b[0m        \u001b[31m1.6286\u001b[0m  2.8459\n","    135       \u001b[36m0.4235\u001b[0m        \u001b[32m1.5612\u001b[0m     \u001b[35m0.3999\u001b[0m        \u001b[31m1.6276\u001b[0m  2.7364\n","    136       \u001b[36m0.4241\u001b[0m        \u001b[32m1.5596\u001b[0m     0.3998        \u001b[31m1.6265\u001b[0m  2.7303\n","    137       \u001b[36m0.4244\u001b[0m        \u001b[32m1.5580\u001b[0m     \u001b[35m0.4007\u001b[0m        \u001b[31m1.6255\u001b[0m  2.7433\n","    138       \u001b[36m0.4253\u001b[0m        \u001b[32m1.5564\u001b[0m     \u001b[35m0.4015\u001b[0m        \u001b[31m1.6246\u001b[0m  2.7268\n","    139       \u001b[36m0.4260\u001b[0m        \u001b[32m1.5549\u001b[0m     \u001b[35m0.4017\u001b[0m        \u001b[31m1.6235\u001b[0m  2.7363\n","    140       \u001b[36m0.4267\u001b[0m        \u001b[32m1.5533\u001b[0m     0.4015        \u001b[31m1.6226\u001b[0m  2.8604\n","    141       \u001b[36m0.4279\u001b[0m        \u001b[32m1.5518\u001b[0m     \u001b[35m0.4025\u001b[0m        \u001b[31m1.6216\u001b[0m  2.7393\n","    142       \u001b[36m0.4285\u001b[0m        \u001b[32m1.5503\u001b[0m     \u001b[35m0.4027\u001b[0m        \u001b[31m1.6207\u001b[0m  2.7430\n","    143       \u001b[36m0.4291\u001b[0m        \u001b[32m1.5487\u001b[0m     \u001b[35m0.4031\u001b[0m        \u001b[31m1.6199\u001b[0m  2.7505\n","    144       \u001b[36m0.4299\u001b[0m        \u001b[32m1.5473\u001b[0m     \u001b[35m0.4035\u001b[0m        \u001b[31m1.6190\u001b[0m  2.8501\n","    145       \u001b[36m0.4303\u001b[0m        \u001b[32m1.5458\u001b[0m     \u001b[35m0.4043\u001b[0m        \u001b[31m1.6181\u001b[0m  2.7521\n","    146       \u001b[36m0.4303\u001b[0m        \u001b[32m1.5443\u001b[0m     0.4038        \u001b[31m1.6173\u001b[0m  2.7416\n","    147       \u001b[36m0.4313\u001b[0m        \u001b[32m1.5428\u001b[0m     \u001b[35m0.4044\u001b[0m        \u001b[31m1.6165\u001b[0m  2.7370\n","    148       \u001b[36m0.4320\u001b[0m        \u001b[32m1.5414\u001b[0m     \u001b[35m0.4055\u001b[0m        \u001b[31m1.6157\u001b[0m  2.8485\n","    149       \u001b[36m0.4325\u001b[0m        \u001b[32m1.5399\u001b[0m     \u001b[35m0.4061\u001b[0m        \u001b[31m1.6148\u001b[0m  2.7463\n","    150       \u001b[36m0.4330\u001b[0m        \u001b[32m1.5385\u001b[0m     0.4061        \u001b[31m1.6140\u001b[0m  2.7256\n","    151       \u001b[36m0.4335\u001b[0m        \u001b[32m1.5370\u001b[0m     \u001b[35m0.4063\u001b[0m        \u001b[31m1.6131\u001b[0m  2.7324\n","    152       \u001b[36m0.4340\u001b[0m        \u001b[32m1.5356\u001b[0m     \u001b[35m0.4069\u001b[0m        \u001b[31m1.6123\u001b[0m  2.8335\n","    153       \u001b[36m0.4344\u001b[0m        \u001b[32m1.5342\u001b[0m     \u001b[35m0.4075\u001b[0m        \u001b[31m1.6115\u001b[0m  2.7392\n","    154       \u001b[36m0.4345\u001b[0m        \u001b[32m1.5328\u001b[0m     \u001b[35m0.4082\u001b[0m        \u001b[31m1.6107\u001b[0m  2.7353\n","    155       \u001b[36m0.4348\u001b[0m        \u001b[32m1.5314\u001b[0m     \u001b[35m0.4084\u001b[0m        \u001b[31m1.6100\u001b[0m  2.7315\n","    156       \u001b[36m0.4351\u001b[0m        \u001b[32m1.5301\u001b[0m     \u001b[35m0.4092\u001b[0m        \u001b[31m1.6092\u001b[0m  2.7354\n","    157       \u001b[36m0.4360\u001b[0m        \u001b[32m1.5287\u001b[0m     0.4091        \u001b[31m1.6085\u001b[0m  2.8626\n","    158       \u001b[36m0.4365\u001b[0m        \u001b[32m1.5273\u001b[0m     \u001b[35m0.4093\u001b[0m        \u001b[31m1.6077\u001b[0m  2.7381\n","    159       \u001b[36m0.4373\u001b[0m        \u001b[32m1.5260\u001b[0m     \u001b[35m0.4096\u001b[0m        \u001b[31m1.6070\u001b[0m  2.7308\n","    160       \u001b[36m0.4375\u001b[0m        \u001b[32m1.5246\u001b[0m     \u001b[35m0.4105\u001b[0m        \u001b[31m1.6063\u001b[0m  2.7412\n","    161       \u001b[36m0.4380\u001b[0m        \u001b[32m1.5233\u001b[0m     \u001b[35m0.4114\u001b[0m        \u001b[31m1.6056\u001b[0m  2.8525\n","    162       \u001b[36m0.4387\u001b[0m        \u001b[32m1.5220\u001b[0m     0.4114        \u001b[31m1.6049\u001b[0m  2.7348\n","    163       \u001b[36m0.4393\u001b[0m        \u001b[32m1.5207\u001b[0m     0.4114        \u001b[31m1.6042\u001b[0m  2.7374\n","    164       \u001b[36m0.4396\u001b[0m        \u001b[32m1.5193\u001b[0m     \u001b[35m0.4125\u001b[0m        \u001b[31m1.6035\u001b[0m  2.7399\n","    165       \u001b[36m0.4402\u001b[0m        \u001b[32m1.5180\u001b[0m     \u001b[35m0.4128\u001b[0m        \u001b[31m1.6028\u001b[0m  2.8430\n","    166       \u001b[36m0.4407\u001b[0m        \u001b[32m1.5167\u001b[0m     0.4126        \u001b[31m1.6022\u001b[0m  2.7426\n","    167       \u001b[36m0.4417\u001b[0m        \u001b[32m1.5155\u001b[0m     \u001b[35m0.4131\u001b[0m        \u001b[31m1.6015\u001b[0m  2.7390\n","    168       \u001b[36m0.4423\u001b[0m        \u001b[32m1.5142\u001b[0m     \u001b[35m0.4139\u001b[0m        \u001b[31m1.6009\u001b[0m  2.8880\n","    169       \u001b[36m0.4430\u001b[0m        \u001b[32m1.5129\u001b[0m     \u001b[35m0.4147\u001b[0m        \u001b[31m1.6003\u001b[0m  2.7460\n","    170       \u001b[36m0.4437\u001b[0m        \u001b[32m1.5117\u001b[0m     \u001b[35m0.4151\u001b[0m        \u001b[31m1.5996\u001b[0m  2.7461\n","    171       \u001b[36m0.4439\u001b[0m        \u001b[32m1.5104\u001b[0m     0.4149        \u001b[31m1.5990\u001b[0m  2.7432\n","    172       \u001b[36m0.4445\u001b[0m        \u001b[32m1.5092\u001b[0m     0.4149        \u001b[31m1.5985\u001b[0m  2.8447\n","    173       \u001b[36m0.4452\u001b[0m        \u001b[32m1.5080\u001b[0m     \u001b[35m0.4153\u001b[0m        \u001b[31m1.5979\u001b[0m  2.7496\n","    174       \u001b[36m0.4460\u001b[0m        \u001b[32m1.5067\u001b[0m     0.4151        \u001b[31m1.5973\u001b[0m  2.7456\n","    175       \u001b[36m0.4462\u001b[0m        \u001b[32m1.5055\u001b[0m     0.4153        \u001b[31m1.5967\u001b[0m  2.7496\n","    176       \u001b[36m0.4464\u001b[0m        \u001b[32m1.5043\u001b[0m     \u001b[35m0.4155\u001b[0m        \u001b[31m1.5962\u001b[0m  2.8519\n","    177       \u001b[36m0.4469\u001b[0m        \u001b[32m1.5031\u001b[0m     0.4152        \u001b[31m1.5955\u001b[0m  2.7452\n","    178       \u001b[36m0.4474\u001b[0m        \u001b[32m1.5019\u001b[0m     0.4153        \u001b[31m1.5950\u001b[0m  2.7390\n","    179       \u001b[36m0.4482\u001b[0m        \u001b[32m1.5007\u001b[0m     0.4152        \u001b[31m1.5945\u001b[0m  2.8538\n","    180       \u001b[36m0.4489\u001b[0m        \u001b[32m1.4995\u001b[0m     0.4153        \u001b[31m1.5940\u001b[0m  2.7314\n","    181       \u001b[36m0.4494\u001b[0m        \u001b[32m1.4983\u001b[0m     \u001b[35m0.4156\u001b[0m        \u001b[31m1.5935\u001b[0m  2.7442\n","    182       \u001b[36m0.4499\u001b[0m        \u001b[32m1.4972\u001b[0m     \u001b[35m0.4163\u001b[0m        \u001b[31m1.5929\u001b[0m  2.7462\n","    183       \u001b[36m0.4509\u001b[0m        \u001b[32m1.4960\u001b[0m     0.4158        \u001b[31m1.5924\u001b[0m  2.7366\n","    184       \u001b[36m0.4519\u001b[0m        \u001b[32m1.4948\u001b[0m     0.4162        \u001b[31m1.5920\u001b[0m  2.7315\n","    185       \u001b[36m0.4521\u001b[0m        \u001b[32m1.4937\u001b[0m     \u001b[35m0.4165\u001b[0m        \u001b[31m1.5915\u001b[0m  2.7305\n","    186       \u001b[36m0.4527\u001b[0m        \u001b[32m1.4925\u001b[0m     0.4165        \u001b[31m1.5910\u001b[0m  2.7481\n","    187       \u001b[36m0.4531\u001b[0m        \u001b[32m1.4914\u001b[0m     \u001b[35m0.4166\u001b[0m        \u001b[31m1.5905\u001b[0m  2.8384\n","    188       \u001b[36m0.4536\u001b[0m        \u001b[32m1.4903\u001b[0m     \u001b[35m0.4169\u001b[0m        \u001b[31m1.5900\u001b[0m  2.7388\n","    189       \u001b[36m0.4541\u001b[0m        \u001b[32m1.4891\u001b[0m     \u001b[35m0.4171\u001b[0m        \u001b[31m1.5896\u001b[0m  2.7324\n","    190       \u001b[36m0.4544\u001b[0m        \u001b[32m1.4880\u001b[0m     \u001b[35m0.4178\u001b[0m        \u001b[31m1.5891\u001b[0m  2.7400\n","    191       \u001b[36m0.4551\u001b[0m        \u001b[32m1.4869\u001b[0m     \u001b[35m0.4181\u001b[0m        \u001b[31m1.5887\u001b[0m  2.8398\n","    192       \u001b[36m0.4553\u001b[0m        \u001b[32m1.4858\u001b[0m     0.4181        \u001b[31m1.5883\u001b[0m  2.7284\n","    193       \u001b[36m0.4556\u001b[0m        \u001b[32m1.4846\u001b[0m     \u001b[35m0.4185\u001b[0m        \u001b[31m1.5878\u001b[0m  2.7452\n","    194       \u001b[36m0.4559\u001b[0m        \u001b[32m1.4835\u001b[0m     \u001b[35m0.4189\u001b[0m        \u001b[31m1.5874\u001b[0m  2.8535\n","    195       \u001b[36m0.4563\u001b[0m        \u001b[32m1.4824\u001b[0m     \u001b[35m0.4190\u001b[0m        \u001b[31m1.5870\u001b[0m  2.7314\n","    196       \u001b[36m0.4568\u001b[0m        \u001b[32m1.4814\u001b[0m     0.4189        \u001b[31m1.5866\u001b[0m  2.7398\n","    197       \u001b[36m0.4573\u001b[0m        \u001b[32m1.4803\u001b[0m     0.4188        \u001b[31m1.5863\u001b[0m  2.7697\n","    198       \u001b[36m0.4578\u001b[0m        \u001b[32m1.4792\u001b[0m     0.4186        \u001b[31m1.5859\u001b[0m  2.8511\n","    199       \u001b[36m0.4584\u001b[0m        \u001b[32m1.4781\u001b[0m     0.4190        \u001b[31m1.5856\u001b[0m  2.7450\n","    200       \u001b[36m0.4592\u001b[0m        \u001b[32m1.4770\u001b[0m     0.4185        \u001b[31m1.5852\u001b[0m  2.7436\n","    201       \u001b[36m0.4597\u001b[0m        \u001b[32m1.4760\u001b[0m     0.4183        \u001b[31m1.5849\u001b[0m  2.7510\n","    202       \u001b[36m0.4601\u001b[0m        \u001b[32m1.4749\u001b[0m     0.4190        \u001b[31m1.5845\u001b[0m  2.8488\n","    203       \u001b[36m0.4607\u001b[0m        \u001b[32m1.4738\u001b[0m     0.4189        \u001b[31m1.5842\u001b[0m  2.7337\n","    204       \u001b[36m0.4610\u001b[0m        \u001b[32m1.4728\u001b[0m     0.4189        \u001b[31m1.5839\u001b[0m  2.7476\n","    205       \u001b[36m0.4615\u001b[0m        \u001b[32m1.4717\u001b[0m     \u001b[35m0.4193\u001b[0m        \u001b[31m1.5837\u001b[0m  2.7472\n","    206       \u001b[36m0.4617\u001b[0m        \u001b[32m1.4707\u001b[0m     \u001b[35m0.4195\u001b[0m        \u001b[31m1.5833\u001b[0m  2.8340\n","    207       \u001b[36m0.4623\u001b[0m        \u001b[32m1.4696\u001b[0m     0.4195        \u001b[31m1.5830\u001b[0m  2.7359\n","    208       \u001b[36m0.4627\u001b[0m        \u001b[32m1.4686\u001b[0m     0.4194        \u001b[31m1.5827\u001b[0m  2.7366\n","    209       \u001b[36m0.4631\u001b[0m        \u001b[32m1.4675\u001b[0m     0.4192        \u001b[31m1.5824\u001b[0m  2.8420\n","    210       \u001b[36m0.4633\u001b[0m        \u001b[32m1.4665\u001b[0m     \u001b[35m0.4196\u001b[0m        \u001b[31m1.5820\u001b[0m  2.7391\n","    211       \u001b[36m0.4638\u001b[0m        \u001b[32m1.4655\u001b[0m     \u001b[35m0.4204\u001b[0m        \u001b[31m1.5817\u001b[0m  2.7289\n","    212       \u001b[36m0.4644\u001b[0m        \u001b[32m1.4645\u001b[0m     0.4202        \u001b[31m1.5815\u001b[0m  2.7427\n","    213       \u001b[36m0.4648\u001b[0m        \u001b[32m1.4635\u001b[0m     \u001b[35m0.4205\u001b[0m        \u001b[31m1.5813\u001b[0m  2.7316\n","    214       \u001b[36m0.4652\u001b[0m        \u001b[32m1.4624\u001b[0m     \u001b[35m0.4219\u001b[0m        \u001b[31m1.5810\u001b[0m  2.8368\n","    215       \u001b[36m0.4654\u001b[0m        \u001b[32m1.4614\u001b[0m     \u001b[35m0.4225\u001b[0m        \u001b[31m1.5807\u001b[0m  2.7328\n","    216       \u001b[36m0.4655\u001b[0m        \u001b[32m1.4604\u001b[0m     \u001b[35m0.4228\u001b[0m        \u001b[31m1.5804\u001b[0m  2.7440\n","    217       \u001b[36m0.4661\u001b[0m        \u001b[32m1.4595\u001b[0m     0.4220        \u001b[31m1.5802\u001b[0m  2.8435\n","    218       \u001b[36m0.4667\u001b[0m        \u001b[32m1.4585\u001b[0m     0.4217        \u001b[31m1.5800\u001b[0m  2.7354\n","    219       \u001b[36m0.4673\u001b[0m        \u001b[32m1.4575\u001b[0m     0.4225        \u001b[31m1.5797\u001b[0m  2.7406\n","    220       \u001b[36m0.4677\u001b[0m        \u001b[32m1.4565\u001b[0m     \u001b[35m0.4230\u001b[0m        \u001b[31m1.5795\u001b[0m  2.7363\n","    221       \u001b[36m0.4684\u001b[0m        \u001b[32m1.4555\u001b[0m     0.4224        \u001b[31m1.5792\u001b[0m  2.8392\n","    222       \u001b[36m0.4688\u001b[0m        \u001b[32m1.4546\u001b[0m     0.4222        \u001b[31m1.5790\u001b[0m  2.7300\n","    223       \u001b[36m0.4692\u001b[0m        \u001b[32m1.4536\u001b[0m     0.4228        \u001b[31m1.5788\u001b[0m  2.7387\n","    224       \u001b[36m0.4697\u001b[0m        \u001b[32m1.4527\u001b[0m     0.4229        \u001b[31m1.5786\u001b[0m  2.7384\n","    225       \u001b[36m0.4708\u001b[0m        \u001b[32m1.4517\u001b[0m     \u001b[35m0.4236\u001b[0m        \u001b[31m1.5783\u001b[0m  2.8496\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["<class 'skorch.net.NeuralNet'>[initialized](\n","  module_=DenseNet(\n","    (B_000): DNStartBlock(\n","      (B_000): Conv2d(3, 8, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","      (B_001): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (B_002): ReLU(inplace=True)\n","      (B_003): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","    )\n","    (B_001): DNDenseBlock(\n","      (B_000): DNCompositeBlock(\n","        (B_000): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_001): DNCompositeBlock(\n","        (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_002): DNCompositeBlock(\n","        (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_003): DNCompositeBlock(\n","        (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (B_002): DNTransitionBlock(\n","      (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (B_001): ReLU(inplace=True)\n","      (B_002): Conv2d(24, 12, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (B_003): DNDenseBlock(\n","      (B_000): DNCompositeBlock(\n","        (B_000): BatchNorm2d(12, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(12, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_001): DNCompositeBlock(\n","        (B_000): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_002): DNCompositeBlock(\n","        (B_000): BatchNorm2d(20, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(20, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_003): DNCompositeBlock(\n","        (B_000): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(24, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_004): DNCompositeBlock(\n","        (B_000): BatchNorm2d(28, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(28, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_005): DNCompositeBlock(\n","        (B_000): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_006): DNCompositeBlock(\n","        (B_000): BatchNorm2d(36, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(36, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_007): DNCompositeBlock(\n","        (B_000): BatchNorm2d(40, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(40, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (B_004): DNTransitionBlock(\n","      (B_000): BatchNorm2d(44, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (B_001): ReLU(inplace=True)\n","      (B_002): Conv2d(44, 22, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (B_005): DNDenseBlock(\n","      (B_000): DNCompositeBlock(\n","        (B_000): BatchNorm2d(22, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(22, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_001): DNCompositeBlock(\n","        (B_000): BatchNorm2d(26, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(26, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_002): DNCompositeBlock(\n","        (B_000): BatchNorm2d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(30, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_003): DNCompositeBlock(\n","        (B_000): BatchNorm2d(34, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(34, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_004): DNCompositeBlock(\n","        (B_000): BatchNorm2d(38, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(38, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_005): DNCompositeBlock(\n","        (B_000): BatchNorm2d(42, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(42, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_006): DNCompositeBlock(\n","        (B_000): BatchNorm2d(46, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(46, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_007): DNCompositeBlock(\n","        (B_000): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(50, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_008): DNCompositeBlock(\n","        (B_000): BatchNorm2d(54, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(54, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_009): DNCompositeBlock(\n","        (B_000): BatchNorm2d(58, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(58, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_010): DNCompositeBlock(\n","        (B_000): BatchNorm2d(62, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(62, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","      (B_011): DNCompositeBlock(\n","        (B_000): BatchNorm2d(66, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_001): ReLU(inplace=True)\n","        (B_002): Conv2d(66, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (B_003): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (B_004): ReLU(inplace=True)\n","        (B_005): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      )\n","    )\n","    (B_006): DNTransitionBlock(\n","      (B_000): BatchNorm2d(70, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (B_001): ReLU(inplace=True)\n","      (B_002): Conv2d(70, 35, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (B_003): AvgPool2d(kernel_size=2, stride=2, padding=0)\n","    )\n","    (B_007): DNClfBlock(\n","      (B_000): AdaptiveAvgPool2d(output_size=(1, 1))\n","      (B_001): Flatten()\n","      (B_002): Linear(in_features=35, out_features=10, bias=True)\n","    )\n","  ),\n",")"]},"metadata":{"tags":[]},"execution_count":54}]},{"cell_type":"code","metadata":{"id":"ua_mUgFv7uzl","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"INfYLbpZ7u5y","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYhbeNkc7vAB","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsF2y9Kt7vHf","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZW4wkdkD7vM9","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"__D0Iqs8WAg3","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"status":"error","timestamp":1595173382901,"user_tz":-540,"elapsed":750458,"user":{"displayName":"­배정렬","photoUrl":"","userId":"08451514273060784656"}},"outputId":"3311e9ba-1732-4d33-ac49-98a49072e477"},"source":["from src.models.train_model import train_net\n","\n","opt = opt_(model.parameters(), lr)\n","\n","trainer = train_net(model, opt, loss_fn, val_metrics,\n","        train_loader, val_loader, device)\n","trainer.run(train_loader, max_epochs=max_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Epoch 1\n","Train - acc: 0.11 loss: 2.33 \n","Val   - acc: 0.11 loss: 2.33 \n","Epoch 2\n","Train - acc: 0.12 loss: 2.32 \n","Val   - acc: 0.12 loss: 2.32 \n","Epoch 3\n","Train - acc: 0.13 loss: 2.31 \n","Val   - acc: 0.13 loss: 2.31 \n","Epoch 4\n","Train - acc: 0.14 loss: 2.30 \n","Val   - acc: 0.14 loss: 2.29 \n","Epoch 5\n","Train - acc: 0.14 loss: 2.28 \n","Val   - acc: 0.15 loss: 2.28 \n","Epoch 6\n","Train - acc: 0.15 loss: 2.27 \n","Val   - acc: 0.15 loss: 2.27 \n","Epoch 7\n","Train - acc: 0.15 loss: 2.26 \n","Val   - acc: 0.16 loss: 2.25 \n","Epoch 8\n","Train - acc: 0.16 loss: 2.24 \n","Val   - acc: 0.17 loss: 2.24 \n","Epoch 9\n","Train - acc: 0.17 loss: 2.23 \n","Val   - acc: 0.17 loss: 2.23 \n","Epoch 10\n","Train - acc: 0.17 loss: 2.22 \n","Val   - acc: 0.17 loss: 2.22 \n","Epoch 11\n","Train - acc: 0.18 loss: 2.21 \n","Val   - acc: 0.18 loss: 2.21 \n","Epoch 12\n","Train - acc: 0.18 loss: 2.20 \n","Val   - acc: 0.19 loss: 2.20 \n","Epoch 13\n","Train - acc: 0.19 loss: 2.18 \n","Val   - acc: 0.19 loss: 2.19 \n","Epoch 14\n","Train - acc: 0.19 loss: 2.17 \n","Val   - acc: 0.20 loss: 2.17 \n","Epoch 15\n","Train - acc: 0.20 loss: 2.16 \n","Val   - acc: 0.20 loss: 2.16 \n","Epoch 16\n","Train - acc: 0.20 loss: 2.15 \n","Val   - acc: 0.20 loss: 2.15 \n","Epoch 17\n","Train - acc: 0.21 loss: 2.14 \n","Val   - acc: 0.21 loss: 2.14 \n","Epoch 18\n","Train - acc: 0.21 loss: 2.13 \n","Val   - acc: 0.21 loss: 2.13 \n","Epoch 19\n","Train - acc: 0.22 loss: 2.12 \n","Val   - acc: 0.22 loss: 2.12 \n","Epoch 20\n","Train - acc: 0.22 loss: 2.11 \n","Val   - acc: 0.23 loss: 2.11 \n","Epoch 21\n","Train - acc: 0.23 loss: 2.10 \n","Val   - acc: 0.23 loss: 2.10 \n","Epoch 22\n","Train - acc: 0.23 loss: 2.09 \n","Val   - acc: 0.24 loss: 2.09 \n","Epoch 23\n","Train - acc: 0.24 loss: 2.08 \n","Val   - acc: 0.24 loss: 2.08 \n","Epoch 24\n","Train - acc: 0.24 loss: 2.07 \n","Val   - acc: 0.25 loss: 2.07 \n","Epoch 25\n","Train - acc: 0.25 loss: 2.06 \n","Val   - acc: 0.25 loss: 2.07 \n","Epoch 26\n","Train - acc: 0.25 loss: 2.05 \n","Val   - acc: 0.25 loss: 2.06 \n","Epoch 27\n","Train - acc: 0.26 loss: 2.04 \n","Val   - acc: 0.26 loss: 2.05 \n","Epoch 28\n","Train - acc: 0.26 loss: 2.03 \n","Val   - acc: 0.26 loss: 2.04 \n","Epoch 29\n","Train - acc: 0.26 loss: 2.02 \n","Val   - acc: 0.26 loss: 2.03 \n","Epoch 30\n","Train - acc: 0.27 loss: 2.01 \n","Val   - acc: 0.27 loss: 2.02 \n","Epoch 31\n","Train - acc: 0.27 loss: 2.00 \n","Val   - acc: 0.28 loss: 2.01 \n","Epoch 32\n","Train - acc: 0.28 loss: 1.99 \n","Val   - acc: 0.28 loss: 2.00 \n","Epoch 33\n","Train - acc: 0.28 loss: 1.98 \n","Val   - acc: 0.28 loss: 1.99 \n","Epoch 34\n","Train - acc: 0.28 loss: 1.97 \n","Val   - acc: 0.28 loss: 1.98 \n","Epoch 35\n","Train - acc: 0.29 loss: 1.97 \n","Val   - acc: 0.29 loss: 1.98 \n","Epoch 36\n","Train - acc: 0.29 loss: 1.96 \n","Val   - acc: 0.29 loss: 1.97 \n","Epoch 37\n","Train - acc: 0.30 loss: 1.95 \n","Val   - acc: 0.29 loss: 1.96 \n","Epoch 38\n","Train - acc: 0.30 loss: 1.94 \n","Val   - acc: 0.29 loss: 1.95 \n","Epoch 39\n","Train - acc: 0.30 loss: 1.93 \n","Val   - acc: 0.30 loss: 1.94 \n","Epoch 40\n","Train - acc: 0.30 loss: 1.92 \n","Val   - acc: 0.30 loss: 1.94 \n","Epoch 41\n","Train - acc: 0.31 loss: 1.92 \n","Val   - acc: 0.31 loss: 1.93 \n","Epoch 42\n","Train - acc: 0.31 loss: 1.91 \n","Val   - acc: 0.31 loss: 1.92 \n","Epoch 43\n","Train - acc: 0.31 loss: 1.90 \n","Val   - acc: 0.31 loss: 1.91 \n","Epoch 44\n","Train - acc: 0.32 loss: 1.89 \n","Val   - acc: 0.31 loss: 1.90 \n","Epoch 45\n","Train - acc: 0.32 loss: 1.88 \n","Val   - acc: 0.32 loss: 1.90 \n","Epoch 46\n","Train - acc: 0.32 loss: 1.88 \n","Val   - acc: 0.32 loss: 1.89 \n","Epoch 47\n","Train - acc: 0.32 loss: 1.87 \n","Val   - acc: 0.32 loss: 1.88 \n","Epoch 48\n","Train - acc: 0.33 loss: 1.86 \n","Val   - acc: 0.32 loss: 1.87 \n","Epoch 49\n","Train - acc: 0.33 loss: 1.85 \n","Val   - acc: 0.32 loss: 1.87 \n","Epoch 50\n","Train - acc: 0.33 loss: 1.84 \n","Val   - acc: 0.33 loss: 1.86 \n","Epoch 51\n","Train - acc: 0.33 loss: 1.84 \n","Val   - acc: 0.33 loss: 1.85 \n","Epoch 52\n","Train - acc: 0.34 loss: 1.83 \n","Val   - acc: 0.33 loss: 1.84 \n","Epoch 53\n","Train - acc: 0.34 loss: 1.82 \n","Val   - acc: 0.33 loss: 1.84 \n","Epoch 54\n","Train - acc: 0.34 loss: 1.82 \n","Val   - acc: 0.33 loss: 1.83 \n","Epoch 55\n","Train - acc: 0.34 loss: 1.81 \n","Val   - acc: 0.33 loss: 1.82 \n","Epoch 56\n","Train - acc: 0.34 loss: 1.80 \n","Val   - acc: 0.34 loss: 1.82 \n","Epoch 57\n","Train - acc: 0.35 loss: 1.80 \n","Val   - acc: 0.34 loss: 1.81 \n","Epoch 58\n","Train - acc: 0.35 loss: 1.79 \n","Val   - acc: 0.34 loss: 1.81 \n","Epoch 59\n","Train - acc: 0.35 loss: 1.79 \n","Val   - acc: 0.34 loss: 1.80 \n","Epoch 60\n","Train - acc: 0.35 loss: 1.78 \n","Val   - acc: 0.34 loss: 1.80 \n","Epoch 61\n","Train - acc: 0.35 loss: 1.78 \n","Val   - acc: 0.35 loss: 1.79 \n","Epoch 62\n","Train - acc: 0.35 loss: 1.77 \n","Val   - acc: 0.35 loss: 1.79 \n","Epoch 63\n","Train - acc: 0.36 loss: 1.76 \n","Val   - acc: 0.35 loss: 1.78 \n","Epoch 64\n","Train - acc: 0.36 loss: 1.76 \n","Val   - acc: 0.35 loss: 1.78 \n","Epoch 65\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.77 \n","Epoch 66\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.77 \n","Epoch 67\n","Train - acc: 0.36 loss: 1.75 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 68\n","Train - acc: 0.36 loss: 1.74 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 69\n","Train - acc: 0.36 loss: 1.74 \n","Val   - acc: 0.35 loss: 1.76 \n","Epoch 70\n","Train - acc: 0.37 loss: 1.73 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 71\n","Train - acc: 0.37 loss: 1.73 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 72\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.75 \n","Epoch 73\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.74 \n","Epoch 74\n","Train - acc: 0.37 loss: 1.72 \n","Val   - acc: 0.36 loss: 1.74 \n","Epoch 75\n","Train - acc: 0.37 loss: 1.71 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 76\n","Train - acc: 0.37 loss: 1.71 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 77\n","Train - acc: 0.37 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.73 \n","Epoch 78\n","Train - acc: 0.37 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 79\n","Train - acc: 0.38 loss: 1.70 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 80\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.36 loss: 1.72 \n","Epoch 81\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.36 loss: 1.71 \n","Epoch 82\n","Train - acc: 0.38 loss: 1.69 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 83\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 84\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.71 \n","Epoch 85\n","Train - acc: 0.38 loss: 1.68 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 86\n","Train - acc: 0.38 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 87\n","Train - acc: 0.39 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.70 \n","Epoch 88\n","Train - acc: 0.39 loss: 1.67 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 89\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 90\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 91\n","Train - acc: 0.39 loss: 1.66 \n","Val   - acc: 0.37 loss: 1.69 \n","Epoch 92\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 93\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 94\n","Train - acc: 0.39 loss: 1.65 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 95\n","Train - acc: 0.39 loss: 1.64 \n","Val   - acc: 0.37 loss: 1.68 \n","Epoch 96\n","Train - acc: 0.39 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 97\n","Train - acc: 0.40 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 98\n","Train - acc: 0.40 loss: 1.64 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 99\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.67 \n","Epoch 100\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 101\n","Train - acc: 0.40 loss: 1.63 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 102\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.38 loss: 1.66 \n","Epoch 103\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.66 \n","Epoch 104\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 105\n","Train - acc: 0.40 loss: 1.62 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 106\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 107\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 108\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.65 \n","Epoch 109\n","Train - acc: 0.41 loss: 1.61 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 110\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 111\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 112\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 113\n","Train - acc: 0.41 loss: 1.60 \n","Val   - acc: 0.39 loss: 1.64 \n","Epoch 114\n","Train - acc: 0.41 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.64 \n","Epoch 115\n","Train - acc: 0.41 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 116\n","Train - acc: 0.42 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 117\n","Train - acc: 0.42 loss: 1.59 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 118\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 119\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 120\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.63 \n","Epoch 121\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 122\n","Train - acc: 0.42 loss: 1.58 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 123\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 124\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 125\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 126\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 127\n","Train - acc: 0.42 loss: 1.57 \n","Val   - acc: 0.40 loss: 1.62 \n","Epoch 128\n","Train - acc: 0.42 loss: 1.56 \n","Val   - acc: 0.40 loss: 1.61 \n","Epoch 129\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 130\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 131\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 132\n","Train - acc: 0.43 loss: 1.56 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 133\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 134\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.61 \n","Epoch 135\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 136\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 137\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 138\n","Train - acc: 0.43 loss: 1.55 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 139\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 140\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 141\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 142\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.60 \n","Epoch 143\n","Train - acc: 0.43 loss: 1.54 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 144\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 145\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.41 loss: 1.59 \n","Epoch 146\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 147\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 148\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 149\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 150\n","Train - acc: 0.44 loss: 1.53 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 151\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 152\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.59 \n","Epoch 153\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 154\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 155\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 156\n","Train - acc: 0.44 loss: 1.52 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 157\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 158\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 159\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 160\n","Train - acc: 0.44 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 161\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 162\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 163\n","Train - acc: 0.45 loss: 1.51 \n","Val   - acc: 0.42 loss: 1.58 \n","Epoch 164\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 165\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 166\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 167\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 168\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 169\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 170\n","Train - acc: 0.45 loss: 1.50 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 171\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 172\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 173\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 174\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 175\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 176\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 177\n","Train - acc: 0.45 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.57 \n","Epoch 178\n","Train - acc: 0.46 loss: 1.49 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 179\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 180\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 181\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 182\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 183\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 184\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 185\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 186\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 187\n","Train - acc: 0.46 loss: 1.48 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 188\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 189\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 190\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 191\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 192\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.56 \n","Epoch 193\n","Train - acc: 0.46 loss: 1.47 \n","Val   - acc: 0.43 loss: 1.55 \n"],"name":"stdout"},{"output_type":"stream","text":["Engine run is terminating due to exception: .\n","Engine run is terminating due to exception: .\n","Traceback (most recent call last):\n","Traceback (most recent call last):\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n","    send_bytes(obj)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","  File \"/usr/lib/python3.6/multiprocessing/queues.py\", line 240, in _feed\n","    send_bytes(obj)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 200, in send_bytes\n","    self._send_bytes(m[offset:offset + size])\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 404, in _send_bytes\n","    self._send(header + buf)\n","  File \"/usr/lib/python3.6/multiprocessing/connection.py\", line 368, in _send\n","    n = write(self._handle, buf)\n","BrokenPipeError: [Errno 32] Broken pipe\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"],"name":"stderr"},{"output_type":"stream","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-18-f6d46458769a>\", line 7, in <module>\n","    trainer.run(train_loader, max_epochs=max_epochs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 658, in run\n","    return self._internal_run()\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 722, in _internal_run\n","    self._handle_exception(e)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 437, in _handle_exception\n","    raise e\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 710, in _internal_run\n","    self._fire_event(Events.EPOCH_COMPLETED)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 393, in _fire_event\n","    func(*first, *(event_args + others), **kwargs)\n","  File \"../src/models/train_model.py\", line 21, in log_training_results\n","    evaluator.run(train_loader)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 658, in run\n","    return self._internal_run()\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 722, in _internal_run\n","    self._handle_exception(e)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 437, in _handle_exception\n","    raise e\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 697, in _internal_run\n","    time_taken = self._run_once_on_dataset()\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 772, in _run_once_on_dataset\n","    self._fire_event(Events.ITERATION_COMPLETED)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/engine/engine.py\", line 393, in _fire_event\n","    func(*first, *(event_args + others), **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/torch/autograd/grad_mode.py\", line 15, in decorate_context\n","    return func(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/metrics/metric.py\", line 220, in iteration_completed\n","    self.update(output)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/metrics/metric.py\", line 487, in wrapper\n","    func(self, *args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ignite/metrics/accuracy.py\", line 164, in update\n","    self._num_correct += torch.sum(correct).item()\n","KeyboardInterrupt\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 1823, in showtraceback\n","    stb = value._render_traceback_()\n","AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 1132, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 313, in wrapped\n","    return f(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 358, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","  File \"/usr/lib/python3.6/inspect.py\", line 1490, in getinnerframes\n","    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n","  File \"/usr/lib/python3.6/inspect.py\", line 1452, in getframeinfo\n","    lines, lnum = findsource(frame)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/ultratb.py\", line 182, in findsource\n","    lines = linecache.getlines(file, globals_dict)\n","  File \"/usr/lib/python3.6/linecache.py\", line 47, in getlines\n","    return updatecache(filename, module_globals)\n","  File \"/usr/lib/python3.6/linecache.py\", line 136, in updatecache\n","    with tokenize.open(fullname) as fp:\n","  File \"/usr/lib/python3.6/tokenize.py\", line 454, in open\n","    encoding, lines = detect_encoding(buffer.readline)\n","  File \"/usr/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n","    first = read_or_stop()\n","  File \"/usr/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n","    return readline()\n","KeyboardInterrupt\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m"]}]},{"cell_type":"markdown","metadata":{"id":"CJYQoYeFWApV","colab_type":"text"},"source":["## Test"]},{"cell_type":"code","metadata":{"id":"Er2I72bjiJXe","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZsuAz94ziJqZ","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4NQ5P36ae5e7","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPA6RFmLLugP","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m29760hLLuOi","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Hz34O1UFinR","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SrtUBWcaFiih","colab_type":"code","colab":{}},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"woSchTbzFh0y","colab_type":"text"},"source":[""]}]}